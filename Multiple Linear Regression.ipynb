{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b27354",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression\n",
    "work with more than two explanatory variables\n",
    "up to this point, the models had one numeric and one categorical explanatory variable\n",
    "what changes if you have 2 numeric explanatory variables instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e7cd1a",
   "metadata": {},
   "source": [
    "#### Two Numeric Explanatory Variables\n",
    "two numeric explanatory variables ples a numeric response variable gives three numeric variables to plot\n",
    "\n",
    "scatter plots are designed to show relationships between two numeric variables, so there are two options:\n",
    "    draw a 3D scatter plot (beyond the scope of this course, impossible to interpret the graph because of perspective issues)\n",
    "    draw a 2D scatter plot and use color for the response variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c2165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 2D scatterplot with the hue argument set to mass, plot the two numeric explanatory variables on the x and y axis and color\n",
    "# the points according to the response variable\n",
    "sns.scatterplot(x=\"length_cm\",\n",
    "                y=\"height_cm\",\n",
    "                data=fish,\n",
    "                hue=\"mass_g\")\n",
    "# the colors get darker as moving up and to the right which represents heavier fish\n",
    "\n",
    "# modeling with an extra explanatory variable\n",
    "mdl_mass_vs_both = ols(\"mass_g ~ length_cm + height_cm\", data=fish).fit()\n",
    "print(mdl_mass_vs_both.params)\n",
    "# you'll get a global intercept coefficient and one slope coefficient for each explanatory variable\n",
    "\n",
    "# prediction flow\n",
    "# create a datafram of explanatory values with product \n",
    "from itertools import product\n",
    "\n",
    "length_cm = np.arange(5, 61, 5)\n",
    "height_cm = np.arange(2, 21, 2)\n",
    "\n",
    "p = product(length_cm, height_cm)\n",
    "explanatory_data = pd.DataFrame(p, columns=[\"length_cm\", \"height_cm\"])\n",
    "\n",
    "# then add a column of predictions with assign and predict\n",
    "prediction_data = explanatory_data.assign(mass_g = mdl_mass_vs_both.predict(explanatory_data))\n",
    "print(prediction_data)\n",
    "\n",
    "# plotting, create one scatter plot with the actual data points and another ote with the prediction data points\n",
    "# to avoid duplication, the legend in one of the scatter plot calls can be removed\n",
    "# the prediction data point markers have been changed to squares for clarity \n",
    "sns.scatterplot(x=\"length_cm\",\n",
    "                y=\"height_cm\",\n",
    "                data=fish,\n",
    "                hue=\"mass_g\")\n",
    "\n",
    "sns.scatterplot(x=\"length_cm\",\n",
    "                y=\"height_cm\",\n",
    "                data=prediction_data,\n",
    "                hue=\"mass_g\",\n",
    "                legend=False,\n",
    "                marker=\"s\")\n",
    "# the color of the grid will give a nice overview of how the response variable changes over the plane of explanatory variables\n",
    "\n",
    "# to include an interaction, replace the plus with a times in the formula\n",
    "mdl_mass_vs_both = ols(\"mass_g ~ length_cm * height_cm\", data=fish).fit()\n",
    "print(mdl_mass_vs_both.params)\n",
    "# this will give you one extra slope term for the interaction of the two explanatory variables\n",
    "\n",
    "# the prediction flow will be the same as above, but you'll probably change the name of the model (mdl_mass_vs_both_inter)\n",
    "\n",
    "# the plotting code is the same as above too :) \n",
    "# this time you will see that the square marker points closesly match the color of the circilar data points and that's a great\n",
    "# visual indicator that the model is a good fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7435881",
   "metadata": {},
   "source": [
    "#### More Than Two Explanatory Variables\n",
    "\n",
    "regression models aren't limited to two explanatory variables\n",
    "we'll look at 3 explanatory variables and what happens when you increase that number even further\n",
    "the fish scatter plot showed some grouping of data points, maybe those are based on species\n",
    "you can check that by faceting on species\n",
    "\n",
    "modeling code scales nicely with more variables but the dimensions of the prediction dataset inclease rapidly to account for all the possible combinations\n",
    "visualizing these predictions isn't as useful now because it reaches the limit of visual interpretation so we stick with predicting the response variable instead \n",
    "\n",
    "In addition to using x and y scales for two numeric variables, you can use color for a third numeric variable, and you can use faceting for categorical variables. That's about all you can really do without using something like a heatmap and that has less info about each variable \n",
    "\n",
    "when using 3 explanatory variables, you have a few options for specifying interactions:\n",
    "    --no interactions\n",
    "    --2-way interactions, gives you model coefficients for each pair of variables\n",
    "    --all the interactions, give you three 2-way interactions and the interaction between all three explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb446e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# give each species its own panel with the FacetGrid function from seaborn\n",
    "\n",
    "# first, prepare the grid by specifying the layout, col let's you split by species, col_wrap makes it a 2x2 grid, and palette\n",
    "# is option and can be used to improve the coloring \n",
    "grid = sns.FacetGrid(data=fish, \n",
    "                     col=\"species\",\n",
    "                     hue=\"mass_g\",\n",
    "                     col_wrap=2,\n",
    "                     palette=\"plasma\")\n",
    "\n",
    "# second, map what visualization you want to plot on the grid\n",
    "# in this example, specify a scatter plot with length and height on the x and y axis respectively \n",
    "grid.map(sns.scatterplot, \"length_cm\", \"height_cm\")\n",
    "plt.show()\n",
    "# you'll see the 4 grids, in the example, there's a noticeable strong positive correlation between length and height for each species of fish\n",
    "# the relationship between the explanatory variables and the response is harder to quantify because you can't determine colors as accurately as x and y coordinates\n",
    "# in this case, you can see that as fish get longer and taller, they also get heavier  \n",
    "                     \n",
    "# it can be tricky in include more than 3 numeric variables in a scatter plot\n",
    "# but you can include as many categorical variables as you like when using faceting, but more can make it harder to see an overall picture                  \n",
    "                     \n",
    "# modeling doesn't really get harder as you increase the number of explanatory variables\n",
    "# model with no interaction\n",
    "ols(\"mass_g ~ length_cm + height_cm + species + 0\", data=fish).fit()\n",
    "                     \n",
    "# two-way (or pairwise) interaction between pairs of variables\n",
    "ols(\"mass_g ~ length_cm + height_cm + species + length_cm:height_cm + height_cm:species + 0\", data=fish).fit()\n",
    "                     \n",
    "# three-way interaction between all the explanatory variables\n",
    "ols(\"mass_g ~ length_cm + height_cm + species + \n",
    "    length_cm:height_cm + length_cm:species + height_cm:species + length_cm:height_cm:species + 0\", data=fish).fit()\n",
    "    \n",
    "# this stuff gets crazy to write so there's shortcuts, swap the plus for times, the above is the same as:\n",
    "ols(\"mass_g ~ length_cm * height_cm * species + 0\", data=fish).fit()\n",
    "    \n",
    "# to get only two-way interaction in the model but not the 3-way\n",
    "ols(\"mass_g ~ (length_cm + height_cm + species) ** 2 + 0\", data=fish).fit()\n",
    "    \n",
    "# prediction flow is the same as normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccadaff",
   "metadata": {},
   "source": [
    "#### How Linear Regression Works\n",
    "\n",
    "to keep it simple, this will be simple linear regression with a single numeric explanatory variable\n",
    "\n",
    "residuals are the actual response minus the predicted response, they'll show as red lines on the scatter plot\n",
    "for the best fit, you want those red lines to be as short as possible, we want a metric that measures the size of all those residuals and we want to make that as small as possible\n",
    "\n",
    "adding up all the residuals would be simple but that doesn't work because some of the residuals are negative so those ones would make the total smaller instead of larger\n",
    "\n",
    "instead of that, we'll do the next easiest thing which is to square each residual so they're non-negative and then add that up, this is called the sum of squares\n",
    "the tricky part of this is figuring out which intercept and slope coefficients will result in the smallest sum of squares\n",
    "\n",
    "to solve this problem, we'll use numerical optimization, it means finding the minimum point of a function\n",
    "we could use calculus for this but we'll let Python find the minimum for us using the optimize package from scipy\n",
    "\n",
    "Linear regression minimizes the sum of the squares of the differences between the actual responses and the predicted responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# define a function to calculate the sum of squares metric\n",
    "# accepts the intercept and slope, returns the sum of the squares of residuals\n",
    "# have to use the trick of giving the function a single coeffs argument then extracting the individual intercept and slope\n",
    "def calc_sum_of_squares(coeffs):\n",
    "    intercept, slope = coeffs\n",
    "    # more calculations\n",
    "# call minimize() to find coefficients that minimize this function\n",
    "# pass an initial guess for the coefficients and your sum of squares function\n",
    "minimize(fun=calc_sum_of_squares, x0=0)\n",
    "\n",
    "# example in video\n",
    "# def calc_quadratic(x):\n",
    "#y = x ** 2 - x + 10\n",
    "# return y\n",
    "# minimize(fun=calc_quadratic, x0=3)\n",
    "\n",
    "\n",
    "# exercise code\n",
    "def calc_sum_of_squares(coeffs):\n",
    "    # Unpack coeffs\n",
    "    intercept, slope = coeffs\n",
    "    # Calculate predicted y-values\n",
    "    y_pred = intercept + slope * x_actual\n",
    "    # Calculate differences between y_actual and y_pred\n",
    "    y_diff = y_pred - y_actual\n",
    "    # Calculate sum of squares\n",
    "    sum_sq = np.sum(y_diff ** 2)\n",
    "    # Return sum of squares\n",
    "    return sum_sq\n",
    "\n",
    "# Call minimize on calc_sum_of_squares  \n",
    "print(minimize(fun=calc_sum_of_squares,\n",
    "               x0=[0, 0]))\n",
    "\n",
    "# Compare the output with the ols() call.\n",
    "print(ols(\"price_twd_msq ~ n_convenience\", data=taiwan_real_estate).fit().params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

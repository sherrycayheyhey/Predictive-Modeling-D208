{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7da2bdb9",
   "metadata": {},
   "source": [
    "#### Modeling Real Data\n",
    "\n",
    "modeling real data (not just example datasets) can require some more powerful tools\n",
    "sometimes scikit-learn will be more convenient, other times statsmodels will be your tool of choice so it's good to know how to use both because no one tool can offer the ideal solution for every problem \n",
    "\n",
    "when using real data, it's very important to consider the uncertainty in the model parameters and predictions\n",
    "\n",
    "REMEMBER: Even our best model is never perfect :)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c536fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Scikit-Learn\n",
    "\n",
    "# import\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# initialize a general model\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "# load and shape the data\n",
    "x_raw, y_raw = load_data()\n",
    "x_data = x_raw.reshape(len(y_raw), 1)\n",
    "y_data = y_raw.reshape(len(y_raw), 1)\n",
    "\n",
    "# fit the model to the data, this finds optimal values for a0 and a1 so that the model fits the data\n",
    "model_fit = model.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbce3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions and parameters\n",
    "\n",
    "# it's not needed to make predictions but you can access the fit model parameters using array style of indexing\n",
    "# extract the linear model parameters\n",
    "intercept = model.intercept_[0]\n",
    "slope = model.coef_[0, 0]\n",
    "\n",
    "# use the model to make predictions\n",
    "future_x = 1200\n",
    "futury_y = model.predict(future_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d2451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another great tool for linear models in python is statsmodels\n",
    "\n",
    "# start by loading the data into numpy arrays and the repack them into a pandas DataFrame\n",
    "x, y = load_data()\n",
    "df = pd.DataFrame(dict(times=x_data, distances=y_data))\n",
    "\n",
    "# plot to preview (visualize) the data\n",
    "fig = df.plot('times', 'distances')\n",
    "\n",
    "# use the ols() and fit() methods to build the model from the data\n",
    "model_fit = ols(formula=\"distances ~ times\", data=df).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de2346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncertainty\n",
    "\n",
    "# extract the optimal values for the parameters (a0 and a1)intercept and slope\n",
    "a0 = model_fit.params['Intercept']\n",
    "a1 = model_fit.params['times']\n",
    "\n",
    "# also extract an error/uncertainty value for each parameter, this lets us quantify the uncertainty to expect form the model\n",
    "e0 = model_fit.bse['Intercept']\n",
    "e1 = model_fit.bse['times']\n",
    "\n",
    "intercept = a0\n",
    "slope = a1\n",
    "uncertainty_in_intercept = e0\n",
    "uncertainty_in_slope = e1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a809367",
   "metadata": {},
   "source": [
    "#### The Limits of Predictions\n",
    "\n",
    "the most common source of large errors is trying to make predictions with a model far outside its **domain of validity**\n",
    "\n",
    "this lesson will show two kinds of prediction abuses: interpolation and extrapolation and the related errors that can arise\n",
    "\n",
    "for a graph that looks like it might contain a linear trend with some random noise: you need to pay attention to the step size (how far apart you want to sample points along the x-axis) because if it's large you may be missing something (consider monthly data versus the same data on a daily scale, the daily scale will show many more features so then you'd realize this data is not simply a linear trend with some random noise) \n",
    "if you fit a model to the undersampled data the fit might look pretty good but if you interpolate (making predictions between the monthly boundaries) to the daily data you'd see that the result is really bad \n",
    "it could be better to fit the linear model just to a limited range (domain of times) from March to August of the same year\n",
    "\n",
    "domain of validity\n",
    "the further away you try to extrapolate, the less accurate your model may become, if you go outside that domain of the known data, the residuals can become huge, it's usually not a good idea to extrapolate too far unless you have other experience or domain knowledge to guide you \n",
    "\n",
    "you could extrapolate for just a segment that's linear but then you need to figure out the smallest and largest values of independent variable x that you'll allow your model to be applied to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2520a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrapolation example exercise\n",
    "# one portion of the data looks linear and was used to build the model but extrapolating all the way will result in some huge residuals\n",
    "# you can set a tolerance to figure out where the domain should be (the smallest and largest values)\n",
    "\n",
    "# Compute the residuals, \"data - model\", and determine where [residuals < tolerance]\n",
    "residuals = np.abs(y_data - y_model)\n",
    "tolerance = 100\n",
    "x_good = x_data[residuals < tolerance]\n",
    "\n",
    "# Find the min and max of the \"good\" values, and plot y_data, y_model, and the tolerance range\n",
    "print('Minimum good x value = {}'.format(np.min(x_good)))\n",
    "print('Maximum good x value = {}'.format(np.max(x_good)))\n",
    "fig = plot_data_model_tolerance(x_data, y_data, y_model, tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2a1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolation example exercise\n",
    "# the monthly data looks linear but the daily data shows different, non-linear trends, undersampling can result in missed data\n",
    "# in this case, using it to interpolate the daily data results in large residuals (the RSS is like 30 times bigger!!)\n",
    "\n",
    "# build and fit a model to the df_monthly data\n",
    "model_fit = ols(formula=\"Close ~ DayCount\", data=df_monthly).fit()\n",
    "\n",
    "# Use the model FIT to the MONTHLY data to make a predictions for both monthly and daily data\n",
    "df_monthly['Model'] = model_fit.predict(df_monthly.DayCount)\n",
    "df_daily['Model'] = model_fit.predict(df_daily.DayCount)\n",
    "\n",
    "# Plot the monthly and daily data and model, compare the RSS values seen on the figures\n",
    "fig_monthly = plot_model_with_data(df_monthly)\n",
    "fig_daily = plot_model_with_data(df_daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f128f9",
   "metadata": {},
   "source": [
    "#### Goodness-of-Fit\n",
    "\n",
    "goodness-of-fit is different but related to RSS\n",
    "\n",
    "there's 3 different R's in linear models:\n",
    "* RSS is for building models, used to help you find the optimal values for model parameters and thus the best model\n",
    "there's no perfect model so there will still be some residuals, but how good is the model? there's two common ways to quantify the goodness-of-fit for a linear model:\n",
    "* RMSE is used for evaluating models, the most common way of quantifying the goodness-of-fit\n",
    "* R-Squared will tell how much of the variation in the data is due to linear trend and how much is not, it's a quantitative measure of that ratio \n",
    "\n",
    "RMSE is how much variation is residual\n",
    "R-squared is what fraction of variation is linear \n",
    "\n",
    "when the variation due to linear trend is larger than the variation due to residuals, the model is better\n",
    "\n",
    "the randomness of the residuals can completed mask the linear dependence for a small slope but be relatively  unimportant for a large slope, R-squared captures this effect but RMSE does not \n",
    "\n",
    "a value of 0 would mean none of the variation in the data is predicted by model, if all the data points are close to the line then r-squared will be closer to 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5cb84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE\n",
    "# start with the residuals and compute RSS as before \n",
    "residuals = y_model - y_data\n",
    "RSS = np.sum(np.square(residuals))\n",
    "\n",
    "# divide RSS by the number of residuals, as if to normalize, you'll get the mean of the squared \n",
    "# residuals instead of the sum \n",
    "mean_squared_residuals = np.sum(np.square(residuals)) / len(residuals)\n",
    "\n",
    "# the residuals can be thought of modeling errors so this is called the mean-squared-error, MSE\n",
    "MSE = np.mean(np.square(residuals))\n",
    "# the form of the variance of the residuals\n",
    "\n",
    "# the square root of MSE will get us the root mean squared error, RMSE\n",
    "RMSE = np.sqrt(np.mean(np.square(residuals)))\n",
    "# the normalized variance or standard deviation of the residuals \n",
    "RMSE = np.std(residuals)\n",
    "# RMSE is a measure of how much the model deviates form the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d4a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-_Squared\n",
    "# deviations are the difference between the data points and the data mean\n",
    "deviations = np.mean(y_data) - y_data\n",
    "# if you square and then sum the deviations you'll get the variance \n",
    "VAR = np.sum(np.square(deviations))\n",
    "# var captures all variation in the data, both the linear trend and the randomness \n",
    "\n",
    "# residuals are the difference between the data points and the model\n",
    "residuals = y_model - y_data\n",
    "# summing and squaring the residuals gives RSS\n",
    "RSS np.sum(np.square(residuals))\n",
    "# RSS only captures the variations left over after the modeled linear trend is subtracted\n",
    "\n",
    "# R-Squared is 1 - the ratio of VAR divided by RSS\n",
    "r_squared = 1 - (RSS / VAR)\n",
    "# it can also be computed as the correlation of the data and the model \n",
    "r = correlation(y_data, y_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece119e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise code, some of the RMSE code simplified\n",
    "# Build the model and compute the residuals \"model - data\"\n",
    "y_model = model_fit_and_predict(x_data, y_data)\n",
    "residuals = y_model - y_data\n",
    "# Compute the RSS, MSE, and RMSE and print the results\n",
    "RSS = np.sum(np.square(residuals))\n",
    "MSE = RSS/len(residuals)\n",
    "RMSE = np.sqrt(MSE)\n",
    "print('RMSE = {:0.2f}, MSE = {:0.2f}, RSS = {:0.2f}'.format(RMSE, MSE, RSS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7832d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise code, r-squared\n",
    "# Compute the residuals and the deviations\n",
    "residuals = y_model - y_data\n",
    "deviations = np.mean(y_data) - y_data\n",
    "\n",
    "# Compute the variance of the residuals and deviations\n",
    "var_residuals = np.sum(np.square(residuals))\n",
    "var_deviations = np.sum(np.square(deviations))\n",
    "\n",
    "# Compute r_squared as 1 - the ratio of RSS/Variance\n",
    "r_squared = 1 - (var_residuals / var_deviations)\n",
    "print('R-squared is {:0.2f}'.format(r_squared))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3350937",
   "metadata": {},
   "source": [
    "#### Standard Error \n",
    "\n",
    "so far you computed quantitative measures of variation and goodness of the model *predictions* but now we'll look at the variation or errors in the model *parameters* \n",
    "How accurate are the model parameters? Are there variation in those parameters? How much of the variation is due to deterministic trends versus inherent randomness? \n",
    "\n",
    "instead of using a single value like RMSE that summarizes the entire model prediction, we'll compute the standard error of each of tha model parameters separately \n",
    "\n",
    "standard error is a measure of the uncertainty in the model parameter values computed in the least-squares process, the parameter value as the center (the slope is the mean of the speed that was traveled turing the marathon) \n",
    "you can now think of those optimal parameter values as no the one true answer but as the best estimate\n",
    "\n",
    "computer the standard error by hand can be tough or like not even possible, right now we'll use statsmodels to easily compute it \n",
    "\n",
    "to truly understand these uncertainties in our models we have to stop thinking of them as errors and begin thinking of them as probability distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b504e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing standard errors\n",
    "# build and fit a model and then extract model parameters\n",
    "# start with a pandas dataframe\n",
    "df = pd.DataFrame(dict(times=x_data, distances=y_data))\n",
    "# ...and pass it into ols() from statsmodels, calling fit\n",
    "model_fit = ols(formula=\"distances ~ times\", data=df).fit()\n",
    "\n",
    "# use dictionary style keyed indexing to get the wanted params\n",
    "a1 = model_fit.params['times']\n",
    "a0 = model_fit.params[\"Intercept\"]\n",
    "slope = a0\n",
    "intercept = a1\n",
    "\n",
    "# the standard error params\n",
    "e0 = model_fit.bse[\"Intercept\"]\n",
    "e1 = model_fit.bse[\"times\"]\n",
    "standard_error_of_intercept = e0\n",
    "standard_error_of_slope = e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d060371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise example \n",
    "# the standard error is the same for both models, but the r-squared changes. \n",
    "# The uncertainty in the estimates of the model parameters is indepedent from R-squred because \n",
    "that uncertainty is being driven not by the linear trend, but by the inherent randomness in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

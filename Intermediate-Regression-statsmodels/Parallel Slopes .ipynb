{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f49e40c",
   "metadata": {},
   "source": [
    "### Parallel Slopes Linear Regression\n",
    "parallel slopes regression with one numeric and one categorical explanatory variable, the first step toward conquering multiple linear regression\n",
    "\n",
    "multiple regression is a regression model with more than one explanatory variable\n",
    "including more explanatory variables can give more insight into the relationship between the explanatory variables and the response and can also provide more accurate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2452a235",
   "metadata": {},
   "source": [
    "#### ONE EXPLANATORY VARIABLE AT A TIME -- numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0058f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression is run by using ols, passing a formula and a dataframe\n",
    "# the formula has the response variable on the left and the explanatory variable on the right of the tilde \n",
    "# the model is then fit using .fit()\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "mdl_mass_vs_length = ols(\"mass_g ~ length_cm\",\n",
    "                          data=koi).fit()\n",
    "\n",
    "# print the model params\n",
    "print(mdl_mass_vs_length.params)\n",
    "\n",
    "# this will give you 1 intercept coeefficient and 1 slope coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc7b535",
   "metadata": {},
   "source": [
    "#### ONE EXPLANATORY VARIABLE AT A TIME -- categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28bb769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you change the categorical variable to an explanatory variable, such as \"species\" \n",
    "# the coefficients will be a little easier to understand if you use \"plus zero\" to tell statsmodels not to include an intercept in the model\n",
    "# you will instead get one intercept coefficient for each category (one coefficient for each species of fish)\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "mdl_mass_vs_species = ols(\"mass_g ~ species + 0\",\n",
    "                          data=koi).fit()\n",
    "print(mdl_mass_vs_species.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cc5a03",
   "metadata": {},
   "source": [
    "#### BOTH EXPLANATORY VARIABLES AT THE SAME TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e165835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to include both, combine them on the right side of the formula\n",
    "mdl_mass_vs_both = ols(\"mass_g ~ length_cm + species + 0\",\n",
    "                        data=koi).fit()\n",
    "\n",
    "# this will give you one slope coefficient and an intercept coefficient for each category in the categorical variable\n",
    "# if you compare the results of both the \"one variable at a time\" models, you'll see that the results are different"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31717fa3",
   "metadata": {},
   "source": [
    "#### VISUALIZATION -- 1 numeric explanatory variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd4286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the standard visualization for a linear regression with a numeric explanatory variable\n",
    "# a single numeric explanatory variable's prediction will form a single straight line\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.regplot(x=\"length_cm\",\n",
    "            y=\"mass_g\",\n",
    "            data=fish,\n",
    "            ci=None)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125ede4",
   "metadata": {},
   "source": [
    "#### VISUALIZATION -- 1 categorical explanatory variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are a few options for plots of this but the simplest one is a boxplot for each category\n",
    "# the model coefficients are the means of each category which are added with the showmeans argument\n",
    "# a single categorical explanatory variable's predictions are the means of each category\n",
    "\n",
    "sns.boxplot(x=\"species\",\n",
    "            y=\"mass_g\",\n",
    "            data=fish,\n",
    "            showmeans=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2b9abc",
   "metadata": {},
   "source": [
    "#### VISUALIZATION -- both explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a77319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn doesn't have an easy way to plot both the explanatory variables model results so drow the trend lines manually\n",
    "\n",
    "# first extract the model coefficients into separate intercepts and the slope\n",
    "coeffs = mdl_mass_vs_both.params\n",
    "print(coeffs)\n",
    "\n",
    "ic_bream, ic_perch, ic_pike, ic_roach, sl = coeffs\n",
    "\n",
    "# now draw a standard scatter plot with one additional argument: hue\n",
    "\n",
    "sns.scatterplot(x=\"length_cm\",\n",
    "                y=\"mass_g\",\n",
    "                hue=\"species\"\n",
    "                data=fish)\n",
    "# hue will be used when working with a categorical or continuous variable and want to color by its values\n",
    "\n",
    "# use plt.axline for each category\n",
    "plt.axline(xyz=(0, ic_bream), slope=sl, color=\"blue\")\n",
    "plt.axline(xyz=(0, ic_perch), slope=sl, color=\"green\")\n",
    "plt.axline(xyz=(0, ic_pike), slope=sl, color=\"red\")\n",
    "plt.axline(xyz=(0, ic_roach), slope=sl, color=\"orange\")\n",
    "\n",
    "# since all use the same slope, the trend lines will be parallel to each other, thus the nickname\"parallel slopes regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde13c16",
   "metadata": {},
   "source": [
    "#### INTERPRETING PARALLEL SLOPES COEFFICIENTS\n",
    "\n",
    "\"parallel slopes\" cases will give you a numeric and a categorical explanatory variable, what do the coefficients mean?\n",
    "\n",
    "for the numecical variable buddy, for each additional nearby convenience store, the expected house price increases by 0.79 dollars\n",
    "\n",
    "for the categorical variable buddy, each of the categories mean that for a house of that age group (0-15 years) with 0 nearby convenience stores, the expected house price is 9.41 dollars per square meter\n",
    "\n",
    "the model has one slope coefficient and then however mant intercept coefficients (one for each category of the explanatory variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb83732",
   "metadata": {},
   "source": [
    "#### INTERPRETING VISUALIZATIONS\n",
    "\n",
    "a single numeric explanatory variable's prediction will form a single straight line\n",
    "a single categorical explanatory variable's predictions are the means of each category\n",
    "\n",
    "The two plots give very different predictions: one gives a predicted response that increases linearly with a numeric variable; the other gives a fixed response for each category. The only sensible way to reconcile these two conflicting predictions is to incorporate both explanatory variables in the model at once.\n",
    "\n",
    "When it comes to a linear regression model with a numeric and a categorical explanatory variable, seaborn doesn't have an easy, \"out of the box\" way to show the predictions so you have to draw the trend lines manually.\n",
    "\n",
    "When looking at the plot, the parallel slopes let you see that newer houses are on average more expensive than houses older than 15 years. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb71cf8d",
   "metadata": {},
   "source": [
    "### PREDICTING PARALLEL SLOPES\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebccc48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prediction workflow start with choosing values for explanatory variables, store them in a pandas dataframe\n",
    "# the end of the arange is non-inclusive\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "exl_data_length = pd.DataFrame({\"length_cm\": np.arange(5, 61, 5)})\n",
    "print(expl_data_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multiple explanatory variables you need to define multiple columns in your explanatory dataframe\n",
    "# for example, if you want all the combinations of a, b, c, 1, 2 then you can use the product function\n",
    "\n",
    "from itertools import product\n",
    "product([\"a\", \"b\", \"c\"], [1, 2])\n",
    "# the output will be all combinations of its inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff086ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create the explanatory variable lists\n",
    "length_cm = np.arange(5, 61, 5)\n",
    "# for a categorical variable, use pandas' unique() method \n",
    "species = fish[\"species\"].unique()\n",
    "\n",
    "# create a combination of all the elements of these input lists\n",
    "p = product(length_cm, species)\n",
    "\n",
    "# next, turn the product into a pandas df and name the columns\n",
    "expl_data_both = pd.DataFrame(p, columns=['length_cm', 'species'])\n",
    "# the output will be 5cm of each fish species, 10cm of each species, etc. \n",
    "\n",
    "# add predictions to the df\n",
    "# predict mass_g from length only\n",
    "prediction_data_length = expl_data_length.assign(mass_g = mdl_mass_vs_length.predict(expl_data))\n",
    "\n",
    "# for two or more explanatory variables, the code is the same except for the variable naming\n",
    "prediction_data_both = expl_data_both.assign(mass_g = mdl_mass_vs_both.predict(expl_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f988011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can visualize the predictions from the model by adding another scatterplot and setting the data argument to pred_data\n",
    "plt.axline(xyz=(0, ic_bream), slope=sl, color=\"blue\")\n",
    "plt.axline(xyz=(0, ic_perch), slope=sl, color=\"green\")\n",
    "plt.axline(xyz=(0, ic_pike), slope=sl, color=\"red\")\n",
    "plt.axline(xyz=(0, ic_roach), slope=sl, color=\"orange\")\n",
    "\n",
    "sns.scatterplot(x=\"length_cm\", \n",
    "                y=\"mass_g\",\n",
    "                hue=\"species\",\n",
    "                data=fish)\n",
    "\n",
    "sns.scatterplot(x=\"length_cm\", \n",
    "                y=\"mass_g\",\n",
    "                color=\"black\",\n",
    "                data=prediction_data)\n",
    "# black is used to distinguish between predictions and actual data points\n",
    "# the black prediction points will lie on the trend lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f49fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of using a bunch of if/else statements, use numpy's select() function to get values based on conditions\n",
    "# np.select() takes two arguments: a list of conditions and a list of choices, both lists need to be the same length \n",
    "\n",
    "# the conditions list contains a condition statement for each species, returns true or false\n",
    "conditions = [\n",
    "    explanatory_data[\"species\"] == \"Bream\", \n",
    "    explanatory_data[\"species\"] == \"Perch\",\n",
    "    explanatory_data[\"species\"] == \"Pike\",\n",
    "    explanatory_data[\"species\"] == \"Roach\"\n",
    "]\n",
    "\n",
    "# the choices list is the collection of intercepts that were extracted from the model coefficients\n",
    "choices = [ic_bream, ic_perch, ic_pike, ic_roach]\n",
    "\n",
    "# np.select() will then retrieve the corresponding intercept for each of the fish species\n",
    "intercept = np.select(conditions, choices)\n",
    "\n",
    "# the final prediction step is to calculate the response\n",
    "prediction_data = explanatory_data.assign(intercept = np.select(conditions, choices),\n",
    "                                          mass_g = intercept + slope * explanatory_data[\"length_cm\"])\n",
    "print(prediction_data)\n",
    "\n",
    "# there's some negative masses predicted, which isn't a good sign \n",
    "# you can check that you got the right answer by calling predict\n",
    "mdl_mass_vs_both.predict(explanatory_data)\n",
    "# the predictions should be the same numbers as the calculated mass column so they're correct\n",
    "# the explanation is that this model performs poorly for smoll fish lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af008e33",
   "metadata": {},
   "source": [
    "### ASSESSING MODEL PERFORMANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5526dbe",
   "metadata": {},
   "source": [
    "the major benefit of using more than one explanatory variable in a model is that you can sometimes get a better fit than when only using a single explanatory variable\n",
    "in the last course, two metrics used for measuring model performance were coefficient of determination (r-squared value, measures how good the regression's prediction line fits the observed values, larger number is better) and the residual standard error (RSE, the typical size of the redisduals)\n",
    "will these metrics improve when both explanatory variables are included in the model\n",
    "\n",
    "something to be aware of is overfitting, too many explanatory variables can cause overfitting so that the data set is fit perfectly but it no longer reflects the general population, you would know the model was overfit if it performed well on this fish dataset but performed poorly on a different fish dataset\n",
    "\n",
    "another metric, adjusted coefficient of determination, includes a small penalty term for each additional explanatory variable which will compensate for the effect of overfitting so it's a better metric than the plain co of d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the coefficient of determination, use rsquared attribute of the fitted model\n",
    "print(mdl_mass_vs_both.rsquared)\n",
    "# 0 is the worst possible fit and 1 is a perfect fit\n",
    "# you could calculate these for all of them and then see which one is the best (closest to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aac3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusted coefficient of determination\n",
    "print(mdl_mass_vs_both.rsquared_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a46004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual standard error\n",
    "rse_length = np.sqrt(mdl_mass_vs_length.mse_resid)\n",
    "# RSE isn't directly available as an attribute but the MSE (mean squared error) is so you can take the square root of the mse\n",
    "# the lowest value would be the best because that means the mass is typically wrong by that amount (about 100 grams, for ex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "138be19c",
   "metadata": {},
   "source": [
    "### Multiple Logistic Regression\n",
    "\n",
    "time to switch from linear regression to logistic regression\n",
    "\n",
    "to perform a logistic regression in statsmodels, used logit instead of ols\n",
    "\n",
    "when the response variable has 2 possible values, there are four possible outcomes for the model: actual false, actual true, predicted true, predicted false\n",
    "these 4 outcomes can be quantified and visualize using a confusion matrix \n",
    "the confusion matrix lets you calculate metrics like model accuracy, sesitivity, and specificity\n",
    "\n",
    "prediction flow is also similar to before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba045bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use logit to perform logistic regression \n",
    "from statsmodels.formula.api import logit\n",
    "\n",
    "logit(\"response ~ explanatory\", data=dataset).fit()\n",
    "\n",
    "# to extend logistic regression to multiple explanatory variables change the formula, just like linear\n",
    "# you can use a plus to ignore interactions\n",
    "logit(\"response ~ explanatory1 + explanatory2\", data=dataset).fit()\n",
    "# or a times to include interactions\n",
    "logit(\"response ~ explanatory1 * explanatory2\", data=dataset).fit()\n",
    "\n",
    "# use a confusion matrix to visualize and quantify\n",
    "conf_matrix = mdl_logit.pred_table()\n",
    "print(conf_matrix)\n",
    "\n",
    "# prediction flow\n",
    "from itertools import product\n",
    "explanatory1 = some_values\n",
    "explanatory2 = some_values\n",
    "# create the combinations of the explanatory variables \n",
    "p = product(explanatory1, explanatory2)\n",
    "# store the combinations in a dataframe \n",
    "explanatory_data = pd.DataFrame(p, columns=[\"explanatory1\", \"explanatory2\"])\n",
    "# assign a new column of predictions\n",
    "prediction_data = explanatory_data.assign(mass_g = mdl_logit.predict(explanatory_data))\n",
    "# for visualization purposes, you can alse create a column with most likely outcomes\n",
    "# it holds the rounded values of the churn predictions so that you can see <0.5 is won't churn and >=0.5 is churned\n",
    "prediction_data[\"most_likely_outcome\"] = np.round(prediction_data[\"has_churned\"])\n",
    "# draw two scatter plots, one for actual and one for prediction data\n",
    "# scatter plot for the actual churn data\n",
    "sns.scatterplot(...\n",
    "                data=churn,\n",
    "                hue=\"has_churned\",\n",
    "                ...)\n",
    "# scatter plot for the prediction data, colored by most likely outcome\n",
    "sns.scatterplot(...\n",
    "                data=prediction_data,\n",
    "                hue=\"most_likely_outcome\",\n",
    "                ...)\n",
    "# probably want to include Legend=False and the example showed alpha=0.2 too \n",
    "# The reason why the data points lie on or to the right of the diagonal line is due to nature of the data: \n",
    "# the time since the last purchase can't exceed the time since the first purchase. Also notice the pattern of the \n",
    "# prediction data: longer customer relationships and shorter purchase recency predicts lower churns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a67c16e",
   "metadata": {},
   "source": [
    "#### The Logistic Distribution\n",
    "\n",
    "in order to understand logistic regression, you need to know about the logistic distribution \n",
    "\n",
    "the Gaussian or normal distribution is the bell curve (probability density function), made with scipy's norm.pdf() function\n",
    "\n",
    "for regression, we care more about the area under this curve, by integrating the norm.pdf function, calculationg the area underneath it, we get another curve called the cumulative distribution function\n",
    "\n",
    "to get the cumulative distribution function (cdf), call norm.cdf instead of pdf\n",
    "\n",
    "cdf can be thought of as the transformation from the values of x to probabilities \n",
    "\n",
    "if you have a logistically distributed variable, x, and a possible value, xval, that x could take, then the CDF gives the probability that x is less than xval\n",
    "The plot of this has an S-shape, known as a sigmoid curve. An important property of this function is that it takes an input that can be any number from minus infinity to infinity, and returns a value between zero and one\n",
    "\n",
    "the CDF for the logistic distribution is also known as the logistic function, the terms are interchangeable\n",
    "its equation is: cdf(x) = 1 / (1+exp(-x))\n",
    "the inverse CDF is sometimes called the logit function, the terms are interchangeable, logit is also know as the log odds ratio\n",
    "for describing predictions\n",
    "its equation is the logarithm of 1 divided by 1 minus p: inverse_cdf(p)=log(1/(1-p))\n",
    "\n",
    "The logistic function (logistic distribution CDF) has another important property: each x input value is transformed to a unique value. That means that the transformation can be reversed. The logit function is the name for the inverse logistic function, which is also the logistic distribution inverse cumulative distribution function. (All three terms mean exactly the same thing.)\n",
    "The logit function takes values between zero and one, and returns values between minus infinity and infinity.\n",
    "\n",
    "the logistic distribution consists of a whole family of curves specified by the location and scale parameters\n",
    "this allows logistic model prediction curves to have different positions or steepness\n",
    "The logistic CDF is not just a single curve. In the same way that the normal distribution has mean and standard deviation parameters that affect the CDF curve, the logistic distribution has location and scale parameters.\n",
    "How do changes to the parameters change the CDF curve?\n",
    "As location increases, the logistic CDF curve moves rightwards. As scale increases, the steepness of the slope decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian cumulative distribution function, CDF\n",
    "x = np.arange(-4, 4.05, 0.05)\n",
    "\n",
    "gauss_dist = pd.DataFrame({\"x\": x, \n",
    "                           \"gauss_pdf\": norm.pdf(x),\n",
    "                           \"gauss_cdf\": norm.cdf(x)})\n",
    "\n",
    "sns.lineplot(x=\"x\",\n",
    "             y=\"gauss_cdf\",\n",
    "             data=gauss_dist)\n",
    "# the left side will be close to zero and the right side will be close to one, \n",
    "# when x has its minimum value (-infinity) y will be 0, when x has it's maximum value (infinity) y will be 1\n",
    "# when x is at 1 the CDF curve is at 0.84, so for a normally distributed variable x, the probability that x is than 1 is 84%\n",
    "\n",
    "# you'll also need a way to get back from probabilities to x values so there's the Gaussian \n",
    "# inverse PDF, percent point function, PPF, or quantile function\n",
    "# this example uses a new dataset with probability from nearly 0 to nearly 1\n",
    "p = np.arange(0.001, 1, .001)\n",
    "gauss_dist_inv = pd.DataFrame({\"p\": p,\n",
    "                               \"gauss_inv_cdf\": norm.pdf(p)})\n",
    "# the line plot you'll see will be the same as the GDF plot above but with the x and y axes flipped \n",
    "\n",
    "# logistic probability density function\n",
    "from scipy.stats import logistic\n",
    "\n",
    "x = np.arange(-4, 4.05, 0.05)\n",
    "logistic_dist = pd.DataFrame(x=\"x\",\n",
    "                            y=\"log_pdf\",\n",
    "                            data=logistic_dist)\n",
    "# this will look similar to the Gaussian PDF but the tails at the extreme left and right of the plot are fatter \n",
    "\n",
    "\n",
    "# cumulative distribution function (exercise code)\n",
    "from scipy.stats import logistic\n",
    "# Create x ranging from minus ten to ten in steps of 0.1\n",
    "x = np.arange(-10, 10.1, 0.1)\n",
    "# Create logistic_dist\n",
    "logistic_dist = pd.DataFrame({\"x\": x,\n",
    "                              \"log_cdf\": logistic.cdf(x),\n",
    "                              \"log_cdf_man\": 1 / (1 + np.exp(-x))})\n",
    "# Using logistic_dist, plot log_cdf vs. x\n",
    "sns.lineplot(x=\"x\", \n",
    "             y=\"log_cdf\",\n",
    "             data=logistic_dist)\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# inverse cumulative distribution function\n",
    "# Create p ranging from 0.001 to 0.999 in steps of 0.001\n",
    "p = np.arange(0.001, 1, 0.001)\n",
    "# Create logistic_dist_inv\n",
    "logistic_dist_inv = pd.DataFrame({\"p\": p,\n",
    "                                  \"logit\": logistic.ppf(p),\n",
    "                                  \"logit_man\": np.log(p / (1 - p))})\n",
    "# Using logistic_dist_inv, plot logit vs. p\n",
    "sns.lineplot(x=\"p\",\n",
    "             y=\"logit\",\n",
    "             data=logistic_dist_inv)\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748df723",
   "metadata": {},
   "source": [
    "#### How Logistic Regression Works\n",
    "\n",
    "basically the same way as linear regression!\n",
    "choose a metric that measures how far the predicted responses are from the actual responses and then optimize that metric\n",
    "for linear regression. the metric to optimize was the sum of squares\n",
    "that won't work for logistic regression though because the actual response is always 0 or 1 and the predicted response is between those two values so the sum of squares performs poorly in this case\n",
    "\n",
    "the likelihood metric is a better bet: np.sum(y_pred * y_actual + (1 - y_pred) * (1 - y_actual))\n",
    "with likelihood you want to find the maximum value \n",
    "since the actual response only has two possible values, this equation will simplify in two ways \n",
    "    --when the actual response is 1 the equation for each observation simplifies to the predicted response, y_pred\n",
    "        --y_pred * 1 + (1 - y_pred) * (1 - 1) = y_pred\n",
    "as y_pred increases the metric will increase too and the maximum likelihood occurs when y_pred is 1, the same as the actual value\n",
    "    --when the actual response is 0 the equation simplifies to y_pred * 0 + (1 - y_pred) * (1 - 0) = 1 - y_pred\n",
    "as y_pred decreases the metric increases and the maximum likelihood occurs when y_pred is zero\n",
    "in either case, you get a higher likelihood score when the predicted response is close to the actual response\n",
    "\n",
    "computing likelihood involves adding many very small numbers which leads to errors, log-likelihood is easier and more efficient\n",
    "the only difference is that you take the logarithm of the predicted response terms\n",
    "log_likelihood = np.log(y_pred) * y_actual + np.log(1 - y_pred) * (1 - y_actual)\n",
    "optimizing to find the log-likelihood gives the same coefficients as optimizing to find the likelihood (both equations give the same answer) \n",
    "\n",
    "we want to maximize likelihood but thu optimize package can only minimize functions \n",
    "so you could calculate the negative log-likelihood\n",
    "maximizing log-likelihood is the same as minimizing negative log-likelihood\n",
    "-np.sum(log_likelihoods)\n",
    "\n",
    "now you can write the logistic regression algorithm\n",
    "def calc_neg_log_likelihood(coeffs):\n",
    "    intercept, slope = coeffs\n",
    "    # more calculations\n",
    "the metric function takes the coefficients argument then you extract the intercept and slope from it then do some more calcs\n",
    "\n",
    "find the coefficients that minimize the metric\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "minimize(fun=calc_neg_log_likelihood, x0=[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fc835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXERCISE\n",
    "#since we want to maximize log-likelihood, but minimize() defaults to finding minimum values, it is easier to calculate the negative log-likelihood.\n",
    "def calc_neg_log_likelihood(coeffs):\n",
    "    # Unpack coeffs\n",
    "    intercept, slope = coeffs\n",
    "    # Calculate predicted y-values\n",
    "    y_pred = logistic.cdf(intercept + slope * x_actual)\n",
    "    # Calculate log-likelihood\n",
    "    log_likelihood = np.log(y_pred) * y_actual + np.log(1 - y_pred) * (1 - y_actual)\n",
    "    # Calculate negative sum of log_likelihood\n",
    "    neg_sum_ll = -np.sum(log_likelihood)\n",
    "    # Return negative sum of log_likelihood\n",
    "    return neg_sum_ll\n",
    "  \n",
    "# Call minimize on calc_sum_of_squares  \n",
    "print(minimize(fun=calc_neg_log_likelihood,\n",
    "               x0=[0, 0]))\n",
    "\n",
    "# Compare the output with the logit() call.\n",
    "print(logit(\"has_churned ~ time_since_last_purchase\", data=churn).fit().params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

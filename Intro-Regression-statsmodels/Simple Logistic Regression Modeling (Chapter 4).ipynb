{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do you need logistic regression?\n",
    "The datasets up to this point all had a numeric response variable but sometimes you'll have a binary response variable. An example is 0 for not churn and 1 for churn in a customer database. If you were to run a linear model with x as the time since the last customer purchase and y as the churn data, you would see that some predictions are above 1 and below 0, which are impossible probabilites because you can only churn (1) or not (0). \n",
    "\n",
    "A solution to this problem is to use logistic regression models. Logistic regression models are another type of generalized linear models. It's used when the response variable is logical. Logistic models result in models that follow a logistic curve (S-shaped). To run a logistic regression, use the logit() function from statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import logit\n",
    "mdl_churn_vs_recency_logit = logit(\"has_churned ~ time_since_last_purchase\", \n",
    "                                   data=churn).fit()\n",
    "# this is basically the same thing as fitting a linear regression but the interpretation is a little different\n",
    "# the result will be two coefficients: one is the intercept and the other is\n",
    "# the numerical explanatory variable\n",
    "print(mdl_churn_vs_recency_logit.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the logistic regression predictions to the plot\n",
    "sns.regplot(x=\"time_since_last_purchase\",\n",
    "            y=\"has_churned\",\n",
    "            data=churn,\n",
    "            ci=None,\n",
    "            logistic=True)\n",
    "# you should now have a logistic curve that never goes below 0 or above 1\n",
    "# to interpret this curve, when the time since last is small the probability\n",
    "# of churning is close to 0, when thi time since last purchase is very high \n",
    "# the probability of churn is close to 1\n",
    "# so customers that have recently bought something are less likely to churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring The Explanatory Variables\n",
    "when the response variable is logical all the points lie on the 1 and 0 lines\n",
    "this makes it difficult to see what is happening\n",
    "you want to see how the explanatory variable is distributed on each line\n",
    "this can be solved with a histogram of the explanatory variable, grouped by the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the histograms of time_since_last_purchase split by has_churned\n",
    "sns.displot(data=churn, \n",
    "            x=\"time_since_last_purchase\", \n",
    "            y=\"has_churned\",\n",
    "            col=\"has_churned\"\n",
    "            )\n",
    "\n",
    "plt.show()\n",
    "#you'll see two histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Linear and Logistic Models Together\n",
    "you can graph both trend side by side to see how they differ\n",
    "the linear line will be straight and the logistic line will be curved (S-shaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot has_churned versus time_since_first_purchase as a scatter plot with a red linear \n",
    "# regression trend line (without a standard error ribbon)\n",
    "# Draw a linear regression trend line and a scatter plot of time_since_first_purchase vs. has_churned\n",
    "sns.regplot(data=churn,\n",
    "            x=\"time_since_first_purchase\",\n",
    "            y=\"has_churned\", \n",
    "            ci=None,\n",
    "            line_kws={\"color\": \"red\"})\n",
    "\n",
    "# Draw a logistic regression trend line and a scatter plot of time_since_first_purchase vs. has_churned\n",
    "sns.regplot(x=\"time_since_first_purchase\",\n",
    "            y=\"has_churned\",\n",
    "            data=churn, \n",
    "            ci=None,\n",
    "            line_kws={\"color\": \"blue\"},\n",
    "            logistic=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions and Odds Ratios\n",
    "to make predictions (calculate probabilities)with a logistic model, use the same techniques as for linear models:\n",
    "* create a dataframe of explanatory variable values\n",
    "* add a response column calculated using the predict method\n",
    "* you can then add those predictions onto the plot by creating a scatter plot of the pred data\n",
    "\n",
    "a simpler prediction you can make is the most likely response\n",
    "if the probability of churning is less than 0.5 then the most likely outcome is not to churn\n",
    "if the probability of churning is more than 0.5 then it's more likely they will churn\n",
    "* to calculate, just round the predicted probabilites \n",
    "* plot the most likely outcome by using the prediction data with the numbers just calculated\n",
    "\n",
    "another way to talk about binary responses is with odds ratios (also used in gambling)\n",
    "odds ratio is the probability of something happening divided by the probability that it doesn't: odds_ratio = probability / (1 - probability)\n",
    "* calculate the value\n",
    "* create a new plot (visualizing odds with the original points doesn't make sense)\n",
    "\n",
    "a nice feature is that logistic regression odds ratios is that they'll change linearly with the explanatory variable on the log-scale\n",
    "since this option is available, it means that log odds ratio is another common way of describing logistic regression predictions, \n",
    "the log-odds ratio is also known as the logit (that's the name of the function, too!)\n",
    "\n",
    "each way of describing responses has different benefits\n",
    "-most likely outcome is easiest to understand because it's either \"yes\" or \"no\" but it lacks precision\n",
    "-for probabilities and odds ratios, both are fairly easy to understand but the non-linear predictions make it more difficult to reason about how changes in the explanatory variable will chance the response variable\n",
    "-log odds ratio is difficult to interpret for individual values but the linear relationship with the explanatory variables makes it easy to reason about changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating probabilities of a response\n",
    "mdl_recency = logit(\"has_churned ~ time_since_last_purchase\",\n",
    "                     data = churn).fit()\n",
    "# create the dataframe of explanatory variable values\n",
    "explanatory_data = pd.DataFrame({\"time_since_last purchase\": np.arange(-1, 6,25, 0,25)})\n",
    "\n",
    "# add a response column calculated using the predict method\n",
    "prediction_data = explanatory_data.assign(has_churned = mdl_recency.predict(explanatory_data))\n",
    "\n",
    "# you can then add those predictions onto the plot by creating a scatter plot of the pred data\n",
    "sns.regplot(x=\"time_since_last_purchase\",\n",
    "            y=\"has_churned\",\n",
    "            data=churn,\n",
    "            ci=None,\n",
    "            logistic=True)\n",
    "\n",
    "sns.scatterplot(x=\"time_since_last_purchase\",\n",
    "                y=\"has_churned\",\n",
    "                data=prediction_data,\n",
    "                color=\"red\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# the points will follow the trend line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round the predicted probabilities\n",
    "prediction_data = explanatory_data.assign(\n",
    "                  has_churned = mdl_recency.predict(explanatory_data))\n",
    "prediction_data[\"most_likely_outcome\"] = np.round(prediction_data[\"has_churned\"])\n",
    "\n",
    "# plot the most likely outcome by using the prediction data with the numbers just calculated\n",
    "sns.regplot(x=\"time_since_last_purchase\",\n",
    "            y=\"has_churned\",\n",
    "            data=churn,\n",
    "            ci=None,\n",
    "            logistic=True)\n",
    "\n",
    "sns.scatterplot(x=\"time_since_last_purchase\",\n",
    "                y=\"most_likely_outcome\",\n",
    "                data=prediction_data,\n",
    "                color=\"red\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# the results will show that for the most recently active customes the most likely outcome is \n",
    "# to not churn, otherwise the most likely outcome is to churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the odds ratio\n",
    "prediction_data[\"odds_ratio\"] = prediction_data[\"has_churned\"] /\n",
    "                                (1 - prediction_data[\"has_churned\"])\n",
    "    \n",
    "# create a new plot using seaborn's lineplot function to create a continuous line\n",
    "sns.lineplot(x=\"time_since_last_purchase\", \n",
    "             y=\"odds_ratio\", \n",
    "             data=prediction_data)\n",
    "\n",
    "# add a horizontal line for where odds ratio equals 1\n",
    "plt.axhline(y=1, linestyle=\"dotted\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# the dotted line is at 1 and shows where churning and not churning are equally likely\n",
    "# the bottom left shows predictions below 1, the chance of churning is less than the chance of not churning\n",
    "# the top right shows predictions where churning is more likely than not churning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can see the odds ratios on the log-scale too, add a logarithmic log scale\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "# calculate log odds ratio\n",
    "prediction_data[\"log_odds_ratio\"] = np.log(prediction_data[\"odds_ratio\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying Logistic Regression Fit\n",
    "this is how you can assess the performance of logistic regression models\n",
    "the diagnostic plots drawn for linear models are less useful for logistic models so confusion matrices are used instead\n",
    "\n",
    "a logical response variable leads to four possible outcomes:\n",
    "* predicted false\n",
    "* predicted true\n",
    "* actual false\n",
    "* actual true\n",
    "\n",
    "if we predicted churn and it was true or if we predicted not churn and that was correct, the model did good, false positives and false negatives are bad, counts of each of these outcomes in called a confusion matrix\n",
    "\n",
    "model fit can be quantified with the performance metrics of: \n",
    "* accuracy: the proportion of correct predictions, tn+tp/(tn+fn+fp+tp), the porportion of customers where the model correctly predicted whether or not they churned\n",
    "* sensitivity: the proportion of true positives, tp/(fn+tp), the proportion of customers who churned where the model correctly predicted that they churned\n",
    "* specificity: the proportion of true negatives, tn/(tn+fp), the proportion of customers who didn't churn where the model correctly predicted that they didn't churn\n",
    "in general, higher is better but there is often a tradeoff where increasing spec will degrease sens or increasing sens will decrease specificity \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix: counts of outcomes\n",
    "\n",
    "# first get the actual responses from the has_churned column of the dataset\n",
    "actual_response = churn[\"has_churned\"]\n",
    "# get the predicted responses from the model by calling predict() on the fitted logistic regression\n",
    "# these predicted values are probabilites \n",
    "# to get the most likely outcome, round the values to 0 or 1\n",
    "predicted_response = np.round(mdl_recency.predict())\n",
    "\n",
    "# combine actual and predicted responses in a dataframe\n",
    "outcomes = pd.DataFrame({\"actual_response\": actual_response,\n",
    "                         \"predicted_response\": predicted_response})\n",
    "# use the value_counts method to get the counts of each combination of values\n",
    "print(outcomes.value_counts(sort=False))\n",
    "\n",
    "# where the actual and pred are the same are the correct ones, the others are incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the confusion matrix can also be created automatically with pred_table (returns an array)\n",
    "conf_matrix = mdl_recency.pred_table()\n",
    "print(conf_matrix)\n",
    "\n",
    "# the results will be  true neg, false pos\n",
    "#                     false neg, true pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix by using the mosaic function\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "\n",
    "mosaic(conf_matrix)\n",
    "# the column widths tell the fraction of observations in each category\n",
    "# each column displays the fraction of predicted observations with each value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance metrics\n",
    "# extract values from the conf_matrix\n",
    "TN = conf_matrix[0,0]\n",
    "TP = conf_matrix[1,1]\n",
    "FN = conf_matrix[1,0]\n",
    "FP = conf_matrix[0,1]\n",
    "\n",
    "# accuracy \n",
    "acc = (TN + TP) / (TN + TP + FN + FP)\n",
    "print(acc)\n",
    "\n",
    "# sensitivity\n",
    "sens = TP / (FN + TP)\n",
    "print(sens)\n",
    "\n",
    "# specificity\n",
    "spec = TN / (TN + FP)\n",
    "print(spec)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear regression is always a straight line\n",
    "straight lines have an intercept (y-coord when x is zero)\n",
    "straight lines have a slope (steepness of y, amount y increases if x is increased by one)\n",
    "equation is y=intercept+slope*x\n",
    "\n",
    "sns.regplot() displays a linear regression trend line but has limitations:\n",
    "* you can't access the intercept and slope as variables\n",
    "* you can't work with the model results as variables\n",
    "\n",
    "this is why sometimes you have to manually run a linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a Linear Regression Steps\n",
    "<ol>\n",
    "    <li>import the ols function</li>\n",
    "    <li>create the model object</li>\n",
    "    <li>fit the model</li>\n",
    "    <li>print the parameters of the fitted model</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import ols from statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols \n",
    "#ols is ordinary least squares, a type of regression that is commonly used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ols takes 2 arguments and is a formula\n",
    "# the response variable on the left side of the tilde and the explanatory variable on the right\n",
    "# the data argument takes the DataFrame contianing the variables\n",
    "\n",
    "price_vs_bedrooms = ols(\"response_variable ~ explanatory_variable\", data=dataframe)\n",
    "# this would be \"price ~ bedrooms\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the .fit() method to your freshly created model object\n",
    "price_vs_bedrooms = price_vs_bedrooms.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the Parameters of the Fitted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the model result\n",
    "# use the .params attribute because it contains the model's parameters\n",
    "# the result will be 2 coefficients: the intercept and slope of the line\n",
    "print(price_vs_bedrooms.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on these results, we expect the price to be:\n",
    "# total_payment_sek = intercept + slope * explanatory variable\n",
    "# this means that for every additional explanatory varible, we expect the house price to increase by the slope\n",
    "\n",
    "the intercept means that on average, a house with 0 bedrooms had a price of (intercept)\n",
    "\n",
    "the explanatory coefficient (slope) result means that if you increase the number of bedrooms\n",
    "by 1, then the expected increases in house price is (slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a histogram will be used to visualize categorical explanatory variables\n",
    "# visualize numeric versus categorical\n",
    "sns.displot(data=flowers, x=\"the histogram to plot, something like price\", col=\"what you want to split the plot by\", bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate means by category\n",
    "mean_price_by_age = dataset.groupby(\"the grouping you want\")[\"what you want the mean of\"].mean()\n",
    "print(mean_price_by_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression with a categorical explanatory variable\n",
    "# same code as numeric explanatory variables, the ruterned coefficients are a little different though\n",
    "# create the model and fit it\n",
    "something_vs_something = ols(\"response variable ~ explanatory variable\"data=dataset).fit()\n",
    "print(something_vs_something.params)\n",
    "# results will showe the the first group as the coefficient and then the other groups relative to it\n",
    "# this is good when dealing with multiple explanatory variable but if you just have one add zero to the formula\n",
    "# this makes all the results relative to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "data can be used to make predictions such as focusing on one species of fish (koi), and then comparing the mass to length so that predictions of the mass can be made if given the length of the fish\n",
    "\n",
    "#### The Priciple of Predicting\n",
    "If I set the explanatory variables to these values, what value would the response variable have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# the specific category can be isolated \n",
    "koi = fish[fish[\"species\"] == \"koi\"]\n",
    "print(koi.head())\n",
    "\n",
    "# start out by plotting mass versus length with a scatterplot\n",
    "sns.regplot(x=\"length\", y=\"mass\", data=koi, ci=None)\n",
    "plt.show()\n",
    "\n",
    "# before making predictions, you need to fit the model\n",
    "mass_vs_length = ols(\"response variable ~ explanatory variable\", data=bream).fit()\n",
    "# view the coefficients of the model\n",
    "print(mass_vs_length)\n",
    "\n",
    "# choose some values for the explanatory variables and store them in a pandas dataframe\n",
    "# in this case the only explanatory variable is the length\n",
    "# specify an interval of values with np.arange() with the start and end of the interval as arguments\n",
    "explanatory_data = pd.DataFrame({\"length\": np.arange(20, 41)})\n",
    "\n",
    "# next do the predictions, prints one prediction for each row of the explanatory data\n",
    "print(mass_vs_length.predict(explanatory_data))\n",
    "\n",
    "# since having a single column of predictions isn't that useful, so put them in a df with \n",
    "# the explanatory variables, use assign()\n",
    "explanatory_data = pd.DataFrame({\"length\": np.arange(20, 41)})\n",
    "prediction_data = explanatory_data.assign(mass=mas_vs_lenght.predict(explanatory_data))\n",
    "print(prediction_data)\n",
    "\n",
    "# now you can answer questions like how heavy would a koi that is 20 cm would be \n",
    "\n",
    "# include these predictions on the scatterplot from before, you can use figure() for this\n",
    "fig = plt.figure()\n",
    "sns.regplot(x=\"length\", \n",
    "            y=\"mass\", \n",
    "            data=koi, \n",
    "            ci=None)\n",
    "sns.scatterplot(x=\"length\", \n",
    "                y=\"mass\",\n",
    "                data=prediction_data,\n",
    "                color=\"red\",\n",
    "                marker=\"s\"\n",
    "               )\n",
    "plt.show()\n",
    "\n",
    "# Extrapolating: making predictions outside the range of observed data\n",
    "# all the fish were between 23 and 38cm but the linear model let's us make predictions outside that range\n",
    "# sometimes it doesn't make sense to extrapolate because the value is extreme and the model will perform badly\n",
    "# for example, if we try to check a teeny, tiny fish:\n",
    "teeny_koi = pd.DataFrame({\"length\": [10]}) # a dataframe with a single observation of 10cm\n",
    "pred_teeny_koi = teeny_koi.assign(mass=mas_vs_lenght.predict(teeny_koi))\n",
    "print(pred_teeny_koi)\n",
    "# result is almost -500 grams and this is not physically possible \n",
    "# think of the context of your data to figure out if it's sensible to extrapolate \n",
    "# linear models don't know what's possible in real life so sometimes you get results that don't make any sense\n",
    "# like if you tried to predict price with -1 grocery stores or 2.5 groceries\n",
    "# you need to understand what the data means in order to determine whether a prediction is nonsense or not\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Objects\n",
    "The model objects created by ols can tell a lot of info, working with individual pieces of the model is often more useful than working with the whole model object at once\n",
    "\n",
    "you can extract the coefficients or parameters from your fitted model with the **.params** attribute:\n",
    "    from statsmodels.formula.api import ols\n",
    "    mass_vs_length olss(\"mass ~ length\", data=koi).fit()\n",
    "    print(mass_vs_length.params)\n",
    "    \n",
    "you can use the **.fittedvalues** attribute to access the predictions on the original dataset used to create the model, this is a shortcut of taking the explanatory variable from the dataset and feeding them to the predict function:\n",
    "    print(mass_vs_length.fittedvalues)\n",
    "    \n",
    "residuals are a measure of inaccuracy in the model fit (actual response values - predictied response values), acces with the **.resid** attribute\n",
    "    print(mass_vs_length.resid)\n",
    "    \n",
    "use the **.summary()** method for a printout of the details of the model:\n",
    "    mass_vs_length.summary()\n",
    "    \n",
    "even though you should use .predict(), you could manually calculate the predictions from the model coefficients, response = intercept + slope * explanatory data:\n",
    "    # get the coefficients/parameters\n",
    "    coeffs = price_vs_size.params\n",
    "    # get the intercept and slope\n",
    "    intercept = coeffs[0]\n",
    "    slope = coeffs[1]\n",
    "    # manually calculate the predictions\n",
    "    price = intercept + slope * explanatory_data\n",
    "    # Compare to the results from .predict()\n",
    "    print(price_twd_msq.assign(predictions_auto=mdl_price_vs_conv.predict(explanatory_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression to the Mean\n",
    "regression to the mean is a property of the data, not a type of model\n",
    "linear regression can be used to quantify its effect\n",
    "eventually extreme cases will look like average cases\n",
    "\n",
    "the response value = fitted value + residual(how much the model missed by)(the parts you could explain + the parts you couldn't explain\" with the model)\n",
    "\n",
    "residuals exist due to problems in the model and because of fundamental randomness, extreme cases are often due to randomness but randomness/luck doesn't persist and will run out, you don't want a perfect model because the real world has randomness and you don't want your model to capture that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make a scatterplot of the data\n",
    "fig = plt.figure() # ,enables plot layering\n",
    "sns.scatterplot(x=\"father_height\", \n",
    "                y=\"son_height\", \n",
    "                data=father_son_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a line where the father height and son height are equal, \n",
    "# the first argument is intercept\n",
    "plt.axline(xy1=(150, 150),\n",
    "           slope=1, \n",
    "           linewidth=2,\n",
    "           color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it so one cm on the x-axis is the same as on the y-axis\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n",
    "# if sons always had the same price as their fathers, all the points would lie on this green line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a linear regression line to the plot\n",
    "# ?? this code could go before the above code for axline?\n",
    "sns.regplot(x=\"father_height\", \n",
    "            y=\"son_height\", \n",
    "            data=father_son_data,\n",
    "            ci=None, \n",
    "            line_kws={\"color\": \"black\"})\n",
    "# the black line above the green line means that for very short fathers, their sons are taller than them on average\n",
    "# the black line below the geen line means that for very tall fathers, their sons are shorter than them on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running a model quantifies the predictions of how much taller or shorter the sons will be\n",
    "# run a regression\n",
    "# sons' heights are the response variable and the fathers' heights are the explanatory variable\n",
    "mdl_son_vs_father = ols(\"son_height ~ father_height\", \n",
    "                        data=father_son_data).fit()\n",
    "print(mdl_son_vs_father.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can make predictions\n",
    "# what is the predicted height of a son if the father is 190cm?\n",
    "tall_dad = pd.DataFrame({\"father_height\": [190]}) # more values could be added\n",
    "mdl_son_vs_father.predict(tall_dad)\n",
    "# you could also try it with a short dad that's 150cm\n",
    "# in both cases you will see that the extreme value became less extreme in the next generation\n",
    "# this is a perfect example of regression to the mean\n",
    "# for example winning teams will probably do worse the next year and losing ones better\n",
    "# a player with many home runs in a season will likely decline the next because the elevated numbers are difficult to sustain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Variables\n",
    "sometimes the relationship between the explanatory variable and the response variable might not be a straight line and to fit the linear regression model you might need to transform the explanatory and/or response variable \n",
    "for example, the data might look curved, a perch might grow in 3 directions at once so the length cubed might give a better, more linear fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform variable with CUBES\n",
    "# plot mass vs. length cubed\n",
    "# first create an additional column where you calculate the length cubed \n",
    "koi[\"length_cm_cubed\"] = koi[\"length_cm\"] ** 3\n",
    "# next, replace this newly created column in the regplot call\n",
    "sns.regplot(x=\"length_cm_cubed\", \n",
    "            y=\"mass_g\", \n",
    "            data=koi,\n",
    "            ci=None)\n",
    "plt.show()\n",
    "\n",
    "# the data points will fit the line much better now so run the model next\n",
    "# to do this, replace the original length variable with the cubed version\n",
    "# then fit the model and extract its coefficients\n",
    "mdl_koi = ols(\"mass_g ~ length_cm_cubed\", data=perch).fit()\n",
    "mdl_koi.params\n",
    "\n",
    "# create the explanatory DataFrame in the same was as usual\n",
    "# insert the cubed length, the untransformed length can be added as reference\n",
    "explanatory_data = pd.DataFrame({\"length_cm_cubed\": np.arange(10, 41, 5) ** 3,\n",
    "                                 \"length_cm\": np.arange(10, 41, 5)})\n",
    "# assign/predict to add predictions\n",
    "prediction_data = explanatory_data.assign(\n",
    "    mass_g=mdl_koi.predict(explanatory_data))\n",
    "print(prediction_data)\n",
    "# these points will be added to the plot and be red squares\n",
    "fig = plt.figure()\n",
    "sns.regplot(x=\"length_cm_cubed\", \n",
    "            y=\"mass_g\", \n",
    "            data=koi,\n",
    "            ci=None)\n",
    "sns.scatterplot(data=prediction_data,\n",
    "                x=\"length_cm_cubed\", \n",
    "                y=\"mass_g\", \n",
    "                color=\"red\", \n",
    "                marker=\"square\")\n",
    "# you can run this plot again but with x=\"length_cm\" for both to see the original x-axis\n",
    "# the linear model will have non-linear predictions after the transformation is undone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform variables with SQUARE ROOT\n",
    "# sometimes a plot can be cramped, like to the bottom left\n",
    "# square roots are a common transformation for data with a right-skewed distribution\n",
    "# this makes it difficult to assess whether there is a good fit\n",
    "sns.regplot(x=\"spent_usd\", \n",
    "            y=\"n_impressions\",\n",
    "            data=ad_conversion,\n",
    "            ci=None)\n",
    "# if you transform both variables by square roots, the data are more spread out \n",
    "# throughout the plot and you can see the points follow the line fairly closely\n",
    "ad_conversion[\"sqrt_spendt_usd\"] = np.sqrt(ad_convestion[\"spent_usd\"])\n",
    "ad_conversion[\"sqrt_n_impressions\"] = np.sqrt(ad_convestion[\"n_impressions\"])\n",
    "sns.regplot(x=\"sqrt_spent_usd\", \n",
    "            y=\"sqrt_n_impressions\",\n",
    "            data=ad_conversion,\n",
    "            ci=None)\n",
    "# modeling is the same as normal\n",
    "mdl_ad = ols(\"sqrt_n_impressions ~ sqrt_spent_usd\", data=ad_conversion).fit()\n",
    "explanatory_data = pd.DataFrame({\"sqrt_spent_usd\": np.sqrt(np.arange(0, 601, 100)),\n",
    "                                 \"spent_usd\": np.arange(0, 601, 100)})\n",
    "# prediction takes an extra step, because we used the square root of the \n",
    "# response variable, not just the explanatory one, the predict function will \n",
    "# predict the square root of the number of impressions, so undo with squaring\n",
    "# this is called \"back transformation\"\n",
    "prediction_data = explanatory_data.assign(\n",
    "    sqrt_n_impressions=mdl_ad.predict(explanatory_data),\n",
    "    n_impressions=mdl_ad.predict(explanatory_data) ** 2)\n",
    "print(prediction_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

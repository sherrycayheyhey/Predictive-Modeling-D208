{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "889e966a",
   "metadata": {},
   "source": [
    "explore the effects of interactions between explanatory variables, it can allow for more realistic models that can have better predictive power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7b1e0",
   "metadata": {},
   "source": [
    "#### Models for Each Category\n",
    "\n",
    "parallel slopes model used a common slope for each category but that's not always the best option\n",
    "one way to give each species of fish a different slope is to run a separate model for each of them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4fd9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each species with a different slope\n",
    "# split up the dataset into the subsets by filtering for each species and assigning the result to its own variable\n",
    "bream = fish[fish[\"species\"] == \"Bream\"]\n",
    "perch = fish[fish[\"species\"] == \"Perch\"]\n",
    "pike = fish[fish[\"species\"] == \"Pike\"]\n",
    "roach = fish[fish[\"species\"] == \"Roach\"]\n",
    "\n",
    "# now that there's all the subsets, run 4 models, each predicting mass based on length for each species\n",
    "mdl_bream = ols(\"mass_g ~ length_cm\", data=bream).fit()\n",
    "print(mdl_bream.params)\n",
    "\n",
    "mdl_perch = ols(\"mass_g ~ length_cm\", data=perch).fit()\n",
    "print(mdl_perch.params)\n",
    "\n",
    "mdl_pike = ols(\"mass_g ~ length_cm\", data=pike).fit()\n",
    "print(mdl_pike.params)\n",
    "\n",
    "mdl_roach = ols(\"mass_g ~ length_cm\", data=roach).fit()\n",
    "print(mdl_roach.params)\n",
    "\n",
    "# to make predictions with these models, first create a df of explanatory variables\n",
    "# each model has the same explanatory variable so this code only needs to be written once\n",
    "explanatory_data = pd.DataFrame({\"length_cm\": np.arange(5, 61, 5)})\n",
    "print(explanatory_data)\n",
    "\n",
    "# the next step to predicting is to add a column with the assign method, name it after the response, variable, \n",
    "# call predict on the model and add explanatory_data as the argument\n",
    "# the model variable will be different for each since every species has its own model coefficients now\n",
    "# for readability, include the labels for species\n",
    "prediction_data_bream = explanatory_data.assign(mass_g = mdl_bream.predict(explanatory_data), species = \"Bream\")\n",
    "prediction_data_perch = explanatory_data.assign(mass_g = mdl_perch.predict(explanatory_data), species = \"Perch\")\n",
    "prediction_data_pike = explanatory_data.assign(mass_g = mdl_pike.predict(explanatory_data), species = \"Pike\")\n",
    "prediction_data_roach = explanatory_data.assign(mass_g = mdl_roach.predict(explanatory_data), species = \"Roach\")\n",
    "\n",
    "# working with all the separate dataframes for each species is annoying so concatenate the predictions into one\\\n",
    "prediction_data = pd.contat([prediction_data_bream, \n",
    "                              prediction_data_roach, \n",
    "                              prediction_data_perch, \n",
    "                              prediction_data_pike])\n",
    "\n",
    "# the regplot function can no longer be used for visualizing because this is regression models across subsets of a dataset\n",
    "# to visualize this, use seaborn's lmplot function\n",
    "sns.lmplot(x=\"length_cm\",\n",
    "           y=\"mass_g\",\n",
    "           data=fish,\n",
    "           hue=\"species\",\n",
    "           ci=None)\n",
    "plt.show()\n",
    "# the resulting graph will show that each line has its own slope\n",
    "\n",
    "# to sanity check our concatenated predictions, we add them to the plot to see if they align with seaborn's lmplot calculations\n",
    "sns.scatterplot(x=\"length_cm\",\n",
    "                y=\"mass_g\",\n",
    "                data=prediction_data,\n",
    "                hue=\"species\",\n",
    "                ci=None,\n",
    "                legend=False)\n",
    "plt.show()\n",
    "# as predicted, each line of prediction points follows seaborn's trend lines\n",
    "\n",
    "# the next question is: are these models better?\n",
    "# calculate the coefficient of determination for a model on the whole fish dataset\n",
    "mdl_fish = ols(\"mass_g ~ length_cm + species\", data=fish).fit()\n",
    "print(mdl_fish.rsquared_adj)\n",
    "# calculate the co of det for each of the individual models\n",
    "print(mdl_bream.rsquared_adj)\n",
    "print(mdl_perch.rsquared_adj)\n",
    "print(mdl_pike.rsquared_adj)\n",
    "print(mdl_roach.rsquared_adj)\n",
    "# one will be higher which indicates a better fit, any that are lower will not be an improvement \n",
    "\n",
    "# calculate the residual standard error for the whole dataset model\n",
    "print(np.sqrt(mdl_fish.mse_resid))\n",
    "# ...and for the individual models\n",
    "print(np.sqrt(mdl_bream.mse_resid))\n",
    "print(np.sqrt(mdl_perch.mse_resid))\n",
    "print(np.sqrt(mdl_pike.mse_resid))\n",
    "print(np.sqrt(mdl_roach.mse_resid))\n",
    "# one of the residual standard errors will be higher which indicates larger differences between actual and predicted values\n",
    "# any models that are lower than the whole dataset model are considered an improvement\n",
    "\n",
    "# a mixed performance result (one thing being a better fit than the whole but that having a bigger standard error, \n",
    "# while other things improve or don't improve between these two metrics) is common\n",
    "# the whole dataset model benefits from the increased power of more rows of data\n",
    "# but, individual models benefit from not having to satisfy different components of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7375a960",
   "metadata": {},
   "source": [
    "#### One Model with an Interaction\n",
    "\n",
    "using different models for different parts of the dataset is annoying\n",
    "a better solution is to specify a single model that contains intercepts and slopes for each category\n",
    "you can do this by specifying interactions between explanatory variables\n",
    "\n",
    "what is an interaction?\n",
    "for example, different fish species have different mass to length ratios\n",
    "the effect of length on the expected mass is different for different species\n",
    "so, the effect of one explanatory variable on the expected response changes depending on the value of another explanatory variable\n",
    "\n",
    "the formula for no interaction is\n",
    "    response ~ explntry1 + explntry2\n",
    "    \n",
    "the formula to include an interaction between those variables, multiply instead\n",
    "this is the implicit version because you don't write down which interactions are needed, stasmodels figures that out\n",
    "    response_var ~ explntry1 * explntry2\n",
    "    \n",
    "the implicit version is usually best but if you want to explicity document which interactions are included in the model then you can use the explicit version\n",
    "    response ~ explntry1 + explntry2 + explntry1:explntry2\n",
    "    \n",
    "the result of these two is the same so decide if you like brevity, or detail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d5148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the model, the formula used in a model\n",
    "#mdl_mass_vs_both = ols(\"mass_g ~ length_cm * species\", data=fish).fit()\n",
    "#print(mdl_mass_vs_both.params)\n",
    "\n",
    "# the coefficients are difficult to understand, just like the models with the categorical explanatory variable were\n",
    "# the intercept coefficient is the intercept for the first species: bream\n",
    "# the length_cm coefficient is the slope for the bream\n",
    "# the intercept coefficient plus the species[T.Perch] coefficient is the intercept for perch\n",
    "# the length coefficient plus the T.Perch coefficient is the slope for perch\n",
    "\n",
    "# this is a mess, we need to use a harder to read formula to get easier to understand coefficients\n",
    "# easier to understand coefficients\n",
    "#mdl_mass_vs_both_inter = ols(\"mass_g ~ species + species:length_cm + 0\", data=fish).fit()\n",
    "#print(mdl_mass_vs_both_inter.params)\n",
    "# the + 0 removes the global intercept\n",
    "# you'll now get an intercept coefficient and slope coefficient for each species\n",
    "# this code result in the same as making models for each category but you don't have to manage four sets of code\n",
    "\n",
    "# response_var ~ explanatory_var1 + explanatory_var2 + explanatory_var1:explanatory_var2 is probably the updated code needed\n",
    "# the video said to use the other code but the exercies said it was depreciated \n",
    "\n",
    "# Fit a linear regression of price_twd_msq versus n_convenience and house_age_years, using the \"times\" syntax to \n",
    "# implicitly generate an interaction between them.\n",
    "mdl_price_vs_both_inter = ols('price_twd_msq ~ n_convenience * house_age_years', data=taiwan_real_estate).fit()\n",
    "\n",
    "# Fit a linear regression of price_twd_msq versus n_convenience and house_age_years, using the \"colon\" syntax to \n",
    "# explicitly generate an interaction between them.\n",
    "mdl_price_vs_both_inter = ols(\"price_twd_msq ~ n_convenience + house_age_years + n_convenience:house_age_years\", data=taiwan_real_estate).fit()\n",
    "\n",
    "# Fit a linear regression of price_twd_msq versus house_age_years plus an interaction between n_convenience and \n",
    "# house_age_years, and no global intercept, using the taiwan_real_estate dataset.\n",
    "mdl_readable_inter = ols(\"price_twd_msq ~ house_age_years + n_convenience:house_age_years + 0\", data=taiwan_real_estate).fit()\n",
    "# based on the last 3 numbers you can conclude that the expected increase in house price for each nearby convenience store is lowest\n",
    "# for the 30-45 age group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349a6c66",
   "metadata": {},
   "source": [
    "#### Making Predictions with Interactions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a893fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the prediction flow used for a model containing an interaction, this is the same as the parallel slopes model\n",
    "from itertools import product\n",
    "\n",
    "# create an array of numbers\n",
    "length_cm = np.arange(5, 61, 5)\n",
    "# extract the unique values of species\n",
    "species = fish[\"species\"].unique()\n",
    "# get all combinations of values of length and species\n",
    "p = product(length_cm, species)\n",
    "# remember: product from itertools is used to get all the combinations of length and species\n",
    "\n",
    "# now convert to a dataframe and name the columns\n",
    "explanatory_data = pd.DataFrame(p, columns=[\"length_cm\", \"species\"])\n",
    "prediction_data = explanatory_data.assign(mass_g = mdl_mass_vs_both_inter.predict(explanatory_data))\n",
    "\n",
    "# visualizing the predictions\n",
    "sns.lmplot(x=\"length_cm\",\n",
    "           y=\"mass_g\",\n",
    "           data=fish,\n",
    "           hue=\"species\",\n",
    "           ci=None)\n",
    "plt.show()\n",
    "# the resulting graph will show that each line has its own slope\n",
    "\n",
    "# to sanity check our concatenated predictions, we add them to the plot to see if they align with seaborn's lmplot calculations\n",
    "sns.scatterplot(x=\"length_cm\",\n",
    "                y=\"mass_g\",\n",
    "                data=prediction_data,\n",
    "                hue=\"species\")\n",
    "plt.show()\n",
    "\n",
    "# to see how the predictions work, you could manually calculate them\n",
    "# get the coefficients from the model using the params attribute\n",
    "coeffs = mdl_mass_vs_both_inter.params\n",
    "# next, unpack all the coefficients into the four intercepts and the four slopes\n",
    "ic_bream, ic_perch, ic_pike, ic_roach, slope_bream, slope_perch, slope_pike, slope_roach = coeffs\n",
    "# now use numpy's select function, it needs a list of conditions and an equally sized list of choices\n",
    "conditions = [\n",
    "    explanatory_data[\"species\"] == \"Bream\", \n",
    "    explanatory_data[\"species\"] == \"Perch\",\n",
    "    explanatory_data[\"species\"] == \"Pike\",\n",
    "    explanatory_data[\"species\"] == \"Roach\"\n",
    "]\n",
    "# match up the correct stuff\n",
    "ic_choices = [ic_bream, ic_perch, ic_pike, ic_roach] #ic is probably short for intercept \n",
    "intercept = np.select(conditions, ic_choices)\n",
    "# match up the correct stuff for the slopes \n",
    "slope_choices = [slope_bream, slope_perch, slope_pike, slope_roach]\n",
    "slope = np.select(conditions, slope_choices)\n",
    "# finally, calculate the predictions using the formula intercept plus slope times explanatory data\n",
    "prediction_data = explanatory_data.assign(mass_g = intercept + slope * explanatory_data[\"length_cm\"])\n",
    "print(prediction_data)\n",
    "# you'll see that the calculated values are the same as those returned by the predict function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf5783",
   "metadata": {},
   "source": [
    "#### Simpson's Paradox\n",
    "\n",
    "Simpson's paradox is a nonintuitive result that some datasets may face\n",
    "it occurs when the trend of a model on the whole dataset is very different from the trends shown by models on subsets\n",
    "\n",
    "for example, the slope of the whole dataset could be positive but if you look at the slope of each group, they could all be negative \n",
    "\n",
    "because of this, it's helpful to visualize your dataset so plot it! this is extra true if some models give conflicting results\n",
    "also remember: you can't choose the best model in general because it depends on the dataset and the question you're trying to answer\n",
    "you should decide on a question before you start fitting models, articulate a question before you start modeling \n",
    "\n",
    "another example is tests scores vs hours playing video games\n",
    "the whole would show a positive slope but each group would show a negative slope\n",
    "at first it would seem like playing more video games increases a test score\n",
    "BUT, the interpretation changes if you reveal that the groups are based on age of children\n",
    "you would now know that older children score higher on the test and that playing lots of video games is related to a lower score\n",
    "\n",
    "most the time the grouped model contains more insight that you'd miss otherwise\n",
    "the discrepancy between the models may reveal that you need to include more explanatory variables (could wealth be the reason there's less infection rates? proximity of hospitals?) \n",
    "context is really important when deciding which model to use, consider your dataset and what question you are trying to answer\n",
    "\n",
    "extreme examples of Simpson's paradox is rare and usually less obvious\n",
    "you may see a zero slope rather than a complete change in direction\n",
    "it may not appear in every group\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

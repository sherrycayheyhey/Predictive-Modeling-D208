{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57c5cfd",
   "metadata": {},
   "source": [
    "#### Inferential Statistics Concepts\n",
    "\n",
    "in this chapter, we'll treat a model parameter, such as slope, not as a single value but instead as a distribution of values whose mean will give the best value \n",
    "random sampling will be used to estimate the parameted distributions, the distributions will be used to make probabilistic statements about both model parameters and model predictinos \n",
    "\n",
    "a statistic summarizes a distribution, for example the mean temperature or the median height\n",
    "\n",
    "population statistics and sample statistics are not usually the same, for example if sample temperatures were all from summer days then the sample mean would be higher than the population mean computed from a full year\n",
    "\n",
    "to ensure the sample is representative of the population, both having about about the same center and spread, you can randomly draw points from the population using np.choice()\n",
    "\n",
    "to verify that the sample is representative of the population, we plot a histogram of both \n",
    "doing this with raw counts is usually not useful because the sample count is often much smaller than the population \n",
    "if you normalize both population and sample, dividing sample bins by 31 (days) and population bins by 3650, each distribution sums to 1, then the difference in normalized distributions becomes clear \n",
    "\n",
    "the shape of a sample distribution is often used to make inferences about the population distribution\n",
    "if you divide each bin count by the total number of days in the data set the sum across all bins is 1\n",
    "so the sum of the half to the right of the 100 degrees would be about 0.5\n",
    "from this normalized distribution, rather than just stating that there's some uncertainty, we can infer more precisely that there is a 50% probability that any day in August will exceed 100 degrees \n",
    "\n",
    "if you use np.random.choice() again to take a second sample then you'd see that the two samples differ, you could resample 100 times\n",
    "think of any measured data set as a sample randomly drawn from a larger population "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4655dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# population statistics versus sample statistics \n",
    "print(len(month_of_temps), month_of_temps.mean(), month_of_temps.std())\n",
    "print(len(decade_of_temps), decade_of_temps.mean(), decade_of_temps.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403e3912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a random sample from a population\n",
    "month_of_temps = np.random.choice(decade_of_temps, size=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampling as iteration, take 20 different randomly chosen samples \n",
    "num_samples = 20\n",
    "for ns in range(num_samples):\n",
    "    sample = np.random.choice(population, num_pts)\n",
    "    distribution_of_means[ns] = sample.mean()\n",
    "    \n",
    "# sample distribution statistics, characterize the shape of the distribution by taking the mean and st. dev\n",
    "mean_of_means = np.mean(distribution_of_means)\n",
    "stdev_of_means = np.std(distribution_of_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fbb9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example exercise, sample statistics versus population \n",
    "# Compute the population statistics (the mean and st dev)\n",
    "print(\"Population mean {:.1f}, stdev {:.2f}\".format( population.mean(), population.std() ))\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Construct a sample by randomly sampling of 31 points from the population\n",
    "sample = np.random.choice(population, size=31)\n",
    "\n",
    "# Compare sample statistics to the population statistics\n",
    "print(\"    Sample mean {:.1f}, stdev {:.2f}\".format( sample.mean(), sample.std() ))\n",
    "# you'll see that the sample stats are similar to the pop stats\n",
    "# if you were to compute len() of each array, it's very different, the the means are not that much different as you might expect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d70a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example exercise, variation in sample statistics\n",
    "# Initialize two arrays of zeros to be used as containers\n",
    "means = np.zeros(num_samples)\n",
    "stdevs = np.zeros(num_samples)\n",
    "\n",
    "# For each iteration, compute and store the sample mean and sample stdev\n",
    "for ns in range(num_samples):\n",
    "    sample = np.random.choice(population, num_pts)\n",
    "    means[ns] = sample.mean()\n",
    "    stdevs[ns] = sample.std()\n",
    "\n",
    "# Compute and print the mean() and std() for the sample statistic distributions\n",
    "print(\"Means:  center={:>6.2f}, spread={:>6.2f}\".format(means.mean(), means.std()))\n",
    "print(\"Stdevs: center={:>6.2f}, spread={:>6.2f}\".format(stdevs.mean(), stdevs.std()))\n",
    "\n",
    "# if only a single sample of 100 was taken then there could only be a single mean and the st dev of that single value would be 0\n",
    "# each sample will be different because of the random draws\n",
    "# the mean of the means is the estimate for the population mean\n",
    "# the stdev of the means is the measure of the uncurtainty in our estimate of the population mean\n",
    "# this is the same concept as the standard error of the slope seen in linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example exercise, visualize the variation of a statistic\n",
    "# Generate sample distribution and associated statistics\n",
    "means, stdevs = get_sample_statistics(population, num_samples=100, num_pts=1000)\n",
    "\n",
    "# Define the binning for the histograms\n",
    "mean_bins = np.linspace(97.5, 102.5, 51)\n",
    "std_bins = np.linspace(7.5, 12.5, 51)\n",
    "\n",
    "# Plot the distribution of means, and the distribution of stdevs\n",
    "fig = plot_hist(data=means, bins=mean_bins, data_name=\"Means\", color='green')\n",
    "fig = plot_hist(data=stdevs, bins=std_bins, data_name=\"Stdevs\", color='red')\n",
    "# you'll see that sample statistics have a distribution of values, not just a single value "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3491eb7d",
   "metadata": {},
   "source": [
    "#### Model Estimation and Likelihood\n",
    "\n",
    "you want to model the population but you only have a sample of the data\n",
    "in this lesson use estimation to build models of population distributions from sampple statistics \n",
    "\n",
    "an example: if you measured the distance traveled by a satellite each hour for a week (the gray bars), the build a model of the distribution of distances traveled in each hour for an entire year (the red curve), to build that model, the assumptions are\n",
    "1. the population model is shaped like a Gaussian\n",
    "2. the sample statistics are good estimates for the population model parameters \n",
    "\n",
    "(do the estimatio code below)\n",
    "why do we do this? what is the liklihood that this specific model best predicts our given data? \n",
    "\n",
    "Likelihood vs Probability\n",
    "\n",
    "conditional probability\n",
    ": stated as a question of what is the probability that A occurs, given the condition that B has already occurred? P(outcome A|given B)\n",
    "\n",
    "probability\n",
    ": if the model is given then we ask what is the probability that it outputs any particular data point? P(data|model)\n",
    "\n",
    "likelihood\n",
    ": if the data is given then we ask what is the likelihood that a candidate model could output the particular data we have? L(model|data)\n",
    "\n",
    "\n",
    "if you had two candidate models then you'd want to choose the one that has the greatest likelihood to output the given data \n",
    "\n",
    "how is liklihood computed?\n",
    "start with a gaussian model (red bell curve line thingy) with specific values for its parameters (mu and sigma)\n",
    "with the condition that this model is given, we ask what is the probability that this model would output one particula data point? for a single distance (value on the x-axis), the probability that the model outputs the data is shown as the horizontal green line (y-value where the value on the x-axis and part of the curve meet)\n",
    "repeat that process for every distance in the sample and take the product \n",
    "if the model peak is centered over most of the sample data then the product of probabilities will be large, but if the sample distances are far away from the model peak (out under the wings of the model) then the likelihood that the model could produce the data set is small "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7769d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimation\n",
    "# define gaussian model function, mu and sigma are the center and spread\n",
    "def gaussian_model(x, mu, sigma):\n",
    "    coeff_part = 1 / (np,sqrt(2 * np.pi * sigma**2))\n",
    "    exp_part = np.exp(-(x - mu)**2 / (2 * sigma**2))\n",
    "    return coeff_part * exp_part\n",
    "\n",
    "# compute sample statistics, the sample mean and the sample standard deviation\n",
    "mean = np.mean(sample)\n",
    "stdev = np.std(sample)\n",
    "\n",
    "# model the population using sample statistics, use the sample mean and stdev as estimates of the population parameters (mu and sigma)\n",
    "population_model = gaussian_samodel(sample, mu=mean, sigma=stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aff766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# likelihood from probabilities\n",
    "# in code, we start by using the sample statistics as guesses for the population model parameters (mu and sigma)\n",
    "\n",
    "# guess parameters\n",
    "mu_guess = np.mean(sample_distances)\n",
    "sigma_guess = np.std(sample_distances)\n",
    "\n",
    "# for each sample point compute a probability by passing the distance and guesses for mu and sigma into the model\n",
    "probabilities = np.zeros(len(sample_distances))\n",
    "for n, distance in enumerate(sample_distances):\n",
    "    probabilities[n] = gaussian_model(distance, mu=mu_guess, sigma=sigma_guess)\n",
    "    \n",
    "# take the product of all those probabilities to compute likelihood\n",
    "likelihood = np.product(probs)\n",
    "# it's useful to take the log of the likelihood because it has better numerical properties\n",
    "loglikelihood = np.sum(np.log(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccb0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum likelihood estimation\n",
    "# now repeat the process for an array of models\n",
    "# try 101 guesses, centered on the sample mean\n",
    "low_guess = sample_mean - 2*sample_stdev\n",
    "high_guess = sample_mean + 2*sample_stdev\n",
    "mu_guessus = np.linspace(low_guess, high_guess, 101)\n",
    "\n",
    "# for each iteration we compute loglikelihood which results in an array of 101 loglikelihoods\n",
    "loglikelihood = np.zeros(len(mu_guesses))\n",
    "for n, mu_guesses in enumerate(mu_guesses):\n",
    "    loglikelihoods[n] = compute_loglikelihood(sample_distances, mu=mu_guesses, sigma=sample_stdev)\n",
    "    \n",
    "# find the best guess (the one guess that gives the maximum loglikelihood)\n",
    "max_loglikelihood = np.max(loglikelihoods)\n",
    "best_mu = mu_guesses[loglikelihoods == max_loglikelihood]\n",
    "\n",
    "# plot the 101 loglikelihood values, one for each guess of mu, and the best estimator for the population\n",
    "# mu is the guess that gives the maximum loglikelihood \n",
    "# when the model is gaussian, this mean matches the answer we'd get from least-squares "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3675818",
   "metadata": {},
   "source": [
    "#### Model Uncertainty and Sample Distributions\n",
    "\n",
    "so far, to estimate a model parameter we've assumed a shape of the parameter distribution: least squares assumes a gaussian shape, maximum likelihood estimations requires us to choose a shape so we went with gaussian\n",
    "but what about situations where the distribution shape is unknown? usually you'll only have a single sample and no idea about the shape of the entire population \n",
    "\n",
    "we could use the sample as the model of the population \n",
    "computing the mean of the single sample will give you a guess but you still have no knowledge of the uncertainty of that guess\n",
    "so what we really want is a prediction for what happens when we take the next sample of a population \n",
    "we could use **bootstrap resampling** which lets you resample the sample many times to simulate taking samples from the population\n",
    "each time you resample, you compute a statistic: the mean\n",
    "sampling the sample then results in a distribution of sample statistic values \n",
    "we can use that to predict that a future sample mean will be 100 degrees with a probability of about 95% the mean will occur between 92 and 107 since 95% is the fraction of the area (or counts) between those values  \n",
    "\n",
    "the *replace* used in the code below is like sampling by putting the marble back in the bag between each sigle draw so that each draw will be from the entire pool of original marbles, not putting the marble back in before making your next draw is sampling without replacement \n",
    "sampling without replacement has two issues: you never have the change to draw a repeated marble (the same marble twice in a row), and the sampling method is changing the model after every draw\n",
    "to estimate the shape of a single population model using resampling thon you have to hold the model population constant (meaning you have to put the marble back in the bag between every draw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f630e861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap\n",
    "# assume we have only a sample of daily high temps and \n",
    "# we want to model the August daily highs for a decade, aka predict the next sample draw from that population\n",
    "\n",
    "# use sample as model for population, assign the sample as the model for the unmeasured population\n",
    "population_model = august_daily_highs_for_2017\n",
    "\n",
    "# simulate repeated data acquisitions by resampling the \"model\", resample the sample and comput the means\n",
    "for nr in range(num_resamples):\n",
    "    bootstrap_sample = np.random.choice(population_model, size=resample_size, replace=True)\n",
    "    bootstrap_means[nr] = np.mean(bootstrap_sample)\n",
    "    # the result will be many mean values, a bootstrap sample distribution\n",
    "\n",
    "# compute the mean of the bootstrap resample distribution, compute the mean of the means\n",
    "estimate_temperature = np.mean(bootstrap_means)\n",
    "# the result is our best estimate of the daily high temperature in any August\n",
    "\n",
    "# compute standard deviation of the bootstrap resample distribution\n",
    "estimate_uncertainty = np.std(bootstrap_means)\n",
    "# the standard deviatio of the means is the standard error or uncertainty \n",
    "\n",
    "# Plot the bootstrap resample distribution of means\n",
    "fig = plot_data_hist(bootstrap_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef9b947",
   "metadata": {},
   "source": [
    "#### Model Errors and Randomness\n",
    "\n",
    "we've seen linear model parameters as distributions, spread about some central peak\n",
    "now we'll relate the parameter distributions to the standard error of linear model parameters\n",
    "we'll also check whether our parameter estimates are effected by randomness \n",
    "\n",
    "there are 3 types of errors common to sampling and measurement\n",
    "- measurement error: mistakes make when collecting or recording the data (a broken censor, wrote down measured values incorrectly)\n",
    "- sampling bias: taking draws from one small portion of the population not representative of the rest (temps only from August when it's super hot to represent the whole year\n",
    "- random choice: variation, how do we know that the mean slope from the model fit is not due to just random fluctuations or noise? \n",
    "\n",
    "null hypothesis \n",
    "that question can be restated as: is our effect due to a relationship or due to random chance?\n",
    "does the ordering or grouping of the data cause an effect larger than what could be produced by randomly shuffled data?\n",
    "We interpret the \"zero effect size\" to mean that if we shuffled samples between short and long times, so that two new samples each have a mix of short and long duration trips, and then compute the test statistic, on average it will be zero.\n",
    "\n",
    "this lesson will focus on answering this question with the null hypothesis \n",
    "\n",
    "p-value is the chance that the effect (or speed) we estimated was the result of random variation in the sample, it's a fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2482ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test statistics\n",
    "# group into early and late times (less than or greater than 5 hours)\n",
    "group_short = sample_distances[times < 5]\n",
    "group_long = sample_distances[times > 5]\n",
    "\n",
    "# resample distributions\n",
    "resample_short = np.random.choice(group_short, size=500, replace=True)\n",
    "resample_long = np.random.choice(group_long, size=500, replace=True)\n",
    "\n",
    "# test statistic\n",
    "# as a test statistics we compute the difference in distance between two randomly chose points, one for each group, and repeat 500 times\n",
    "test_statistic = resample_long - resample_short\n",
    "\n",
    "# effect size as mean of test statistic distribution\n",
    "effect_size = np.mean(test_statistic)\n",
    "# we take the mean of these differences to be our effect size for how increasing time effects a change in distance \n",
    "\n",
    "# now reshuffle the data, removing any sense of time grouping, in this example you'll see that the two distributions of shuffled\n",
    "# group distances are entirely overlapped, you can see that the same test will likely average to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7bf0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null hypothesis\n",
    "# shuffle and split\n",
    "shuffle_bucket = np.concatenate((group_short, group_long))\n",
    "np.random.shuffle(shuffle_bucket)\n",
    "\n",
    "# split in the middle, slice the shuffled data into 2\n",
    "slice_index = len(shuffled_backet)//2\n",
    "shuffled_half1 = shuffle_bucket[0:slice_index]\n",
    "shuffled_half2 = shuffle_bucket[slice_index+1:]\n",
    "\n",
    "# resample shuffled populations, resample the arbitrary half of the shuffled data \n",
    "shuffled_sample1 = np.random.choice(shuffled_half1, size=500, replace=True)\n",
    "shuffled_sample2 = np.random.choice(shuffled_half2, size = 500, replace=True)\n",
    "\n",
    "# recompute the test statistic and effect size\n",
    "shuffled_test_statistic = shuffled_sample_2 - shuffled_sample1\n",
    "effect_size = np.mean(shuffled_test_statistic)\n",
    "\n",
    "# plot what, if any, change came from shuffling, plot the two test statistic distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdbc92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing test statistics\n",
    "# From the unshuffled groups, compute the test statistic distribution\n",
    "resample_short = np.random.choice(group_duration_short, size=500, replace=True)\n",
    "resample_long = np.random.choice(group_duration_long, size=500, replace=True)\n",
    "test_statistic_unshuffled = resample_long - resample_short\n",
    "\n",
    "# Shuffle two populations, cut in half, and recompute the test statistic\n",
    "shuffled_half1, shuffled_half2 = shuffle_and_split(group_duration_short, group_duration_long)\n",
    "resample_half1 = np.random.choice(shuffled_half1, size=500, replace=True)\n",
    "resample_half2 = np.random.choice(shuffled_half2, size=500, replace=True)\n",
    "test_statistic_shuffled = resample_half2 - resample_half1\n",
    "\n",
    "# Plot both the unshuffled and shuffled results and compare\n",
    "fig = plot_test_statistic(test_statistic_unshuffled, label='Unshuffled')\n",
    "fig = plot_test_statistic(test_statistic_shuffled, label='Shuffled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7151bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the p-value\n",
    "# the goal is to visualize this as the fraction of points in the shuffled test statistic distribution that fall to the right of\n",
    "# the mean of the test statistic (effect size) computed from the unshuffled samples\n",
    "\n",
    "# Compute the test stat distribution and effect size for two population groups\n",
    "test_statistic_unshuffled = compute_test_statistic(group_duration_short, group_duration_long)\n",
    "effect_size = np.mean(test_statistic_unshuffled)\n",
    "\n",
    "# Randomize the two populations, and recompute the test stat distribution\n",
    "shuffled_half1, shuffled_half2 = shuffle_and_split(group_duration_short, group_duration_long)\n",
    "test_statistic_shuffled = compute_test_statistic(shuffled_half1, shuffled_half2)\n",
    "\n",
    "# Compute the p-value as the proportion of shuffled test stat values >= the effect size\n",
    "condition = test_statistic_shuffled >= effect_size\n",
    "p_value = len(test_statistic_shuffled[condition]) / len(test_statistic_shuffled)\n",
    "\n",
    "# Print p-value and overplot the shuffled and unshuffled test statistic distributions\n",
    "print(\"The p-value is = {}\".format(p_value))\n",
    "fig = plot_test_stats_and_pvalue(test_statistic_unshuffled, test_statistic_shuffled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
